–ó–∞–¥–∞–Ω–∏—è
1. –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä–æ–ª–µ–π
–°–æ–∑–¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: dev1, dev2, ops1.

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–ª—è –Ω–∏—Ö –ø–∞—Ä–æ–ª–∏.

–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –ø–∞—Ä–æ–ª–µ–π.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

2. –†–∞–±–æ—Ç–∞ —Å –≥—Ä—É–ø–ø–∞–º–∏
–°–æ–∑–¥–∞—Ç—å –≥—Ä—É–ø–ø—ã: developers (–¥–æ–±–∞–≤–∏—Ç—å dev1, dev2) –∏ operations (–¥–æ–±–∞–≤–∏—Ç—å ops1).

–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á–ª–µ–Ω—Å—Ç–≤–æ –≤ –≥—Ä—É–ø–ø–∞—Ö.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Å—Ç–∞–≤ –≥—Ä—É–ø–ø.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –≥—Ä—É–ø–ø—ã.

3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º
–°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É /shared/dev_project.

–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–∞–≤–∞:

–ì—Ä—É–ø–ø–∞ developers ‚Äî —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å.

–û—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∞ –∏ –≥—Ä—É–ø–ø—É-–≤–ª–∞–¥–µ–ª—å—Ü–∞.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –ø–∞–ø–∫—É.

4. –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –≥—Ä—É–ø–ø
–£–¥–∞–ª–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è dev2 (—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–º–∞—à–Ω—é—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é).

–£–¥–∞–ª–∏—Ç—å –≥—Ä—É–ø–ø—É operations (–µ—Å–ª–∏ –ø—É—Å—Ç–∞).

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏ –≥—Ä—É–ø–ø–∞ —É–¥–∞–ª–µ–Ω—ã.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –æ—Å—Ç–∞—Ç–∫–∏.

5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ sudo
–†–∞–∑—Ä–µ—à–∏—Ç—å ops1 –≤—ã–ø–æ–ª–Ω—è—Ç—å –ª—é–±—ã–µ –∫–æ–º–∞–Ω–¥—ã —á–µ—Ä–µ–∑ sudo.

–†–∞–∑—Ä–µ—à–∏—Ç—å –≥—Ä—É–ø–ø–µ developers –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ç—å nginx.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∞ sudo.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ /etc/sudoers.

6. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–æ–º
–°–æ–∑–¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ci_user –±–µ–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∞ (/sbin/nologin).

–°–æ–∑–¥–∞—Ç—å –≥—Ä—É–ø–ø—É ci_agents –∏ –¥–æ–±–∞–≤–∏—Ç—å ci_user.

–°–æ–∑–¥–∞—Ç—å /var/ci —Å –ø—Ä–∞–≤–∞–º–∏ 750.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –≥—Ä—É–ø–ø—É –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –≤—Å—ë —Å–æ–∑–¥–∞–Ω–Ω–æ–µ.

7. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∞ (ACL)
–†–∞–∑—Ä–µ—à–∏—Ç—å ops1 —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª /home/dev1/secret.txt —á–µ—Ä–µ–∑ ACL.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ ACL.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç ACL.

8. –ê—É–¥–∏—Ç –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞—É–¥–∏—Ç –¥–µ–π—Å—Ç–≤–∏–π ops1 —á–µ—Ä–µ–∑ auditd.

–°–∫—Ä–∏–ø—Ç—ã:

check.sh ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∞—É–¥–∏—Ç–∞.

cleanup.sh ‚Äî —É–¥–∞–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞.

–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?
–í—ã–ø–æ–ª–Ω–∏—Ç–µ –∑–∞–¥–∞–Ω–∏–µ –≤—Ä—É—á–Ω—É—é –∏–ª–∏ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç.

–ó–∞–ø—É—Å—Ç–∏—Ç–µ ./check.sh –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–ø—É—Å—Ç–∏—Ç–µ ./cleanup.sh.

Practical Tasks: Linux User and Group Management
For DevOps Trainees
This repository contains hands-on tasks for Linux user and group management. Each task includes:

Task description

Verification script (check.sh) ‚Äî validates the solution.

Cleanup script (cleanup.sh) ‚Äî removes created resources.

Tasks
1. User Creation and Password Assignment
Create users: dev1, dev2, ops1.

Set passwords for them.

Verify user creation.

Scripts:

check.sh ‚Äî checks if users and passwords exist.

cleanup.sh ‚Äî removes users.

2. Group Management
Create groups: developers (add dev1, dev2) and operations (add ops1).

Verify group membership.

Scripts:

check.sh ‚Äî checks group members.

cleanup.sh ‚Äî deletes groups.

3. File Permissions
Create /shared/dev_project directory.

Set permissions:

developers group ‚Äî read/write.

Others ‚Äî read-only.

Scripts:

check.sh ‚Äî verifies permissions and ownership.

cleanup.sh ‚Äî removes the directory.

4. User and Group Deletion
Delete user dev2 (keep home directory).

Delete group operations (if empty).

Scripts:

check.sh ‚Äî checks if user/group are deleted.

cleanup.sh ‚Äî cleans up leftovers.

5. Sudo Configuration
Allow ops1 to run any command via sudo.

Allow developers group to restart nginx.

Scripts:

check.sh ‚Äî checks sudo privileges.

cleanup.sh ‚Äî removes sudo rules.

6. Automation with Script
Create ci_user with no login shell (/sbin/nologin).

Create group ci_agents and add ci_user.

Create /var/ci with 750 permissions.

Scripts:

check.sh ‚Äî checks user, group, and directory.

cleanup.sh ‚Äî removes all created items.

7. ACL (Advanced Permissions)
Allow ops1 to read /home/dev1/secret.txt via ACL.

Scripts:

check.sh ‚Äî checks ACL settings.

cleanup.sh ‚Äî removes ACL.

8. User Activity Auditing
Configure auditd to log ops1 actions.

Scripts:

check.sh ‚Äî checks audit rules.

cleanup.sh ‚Äî removes audit rules.

How to Use?
Complete the task manually or via a script.

Run ./check.sh to validate.

After completion, run ./cleanup.sh.

Good luck! üöÄ
--------------------------------------
# 3. Remove Nginx package
echo "Removing Nginx package..."
apt remove --purge -y nginx
if [ $? -eq 0 ]; then
    echo "Nginx removed successfully."
else
    echo "Error removing Nginx package. Manual intervention may be required."
fi

-------------------
Will watch for changes in these directories: ['/home/artsiom/atlantis/atlantis-fastapi']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [30242] using StatReload
INFO:     Started server process [30251]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     127.0.0.1:38624 - "GET / HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:40404 - "GET /clients HTTP/1.1" 307 Temporary Redirect
INFO:     127.0.0.1:40404 - "GET /clients/ HTTP/1.1" 401 Unauthorized

-----------------
from aiogram import Bot, Dispatcher, types
from aiogram.fsm.storage.redis import RedisStorage
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.dispatcher.fsm.context import FSMContext
from aiogram.dispatcher.fsm.state import State, StatesGroup
from db import async_session
from bot.middlewares.db_middleware import DatabaseSessionMiddleware
from bot.middlewares.state_middleware import PydanticStateMiddleware
from bot.schemas.user import UserStateModel
from bot.handlers.start import user_creation_router
from bot.config import config
import asyncio
import boto3
import json

# –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot = Bot(token=config.BOT_TOKEN)
if config.REDIS_ENABLED and config.REDIS_URL:
    dp = Dispatcher(storage=RedisStorage.from_url(config.REDIS_URL))
    print("Redis connected")
else:
    dp = Dispatcher(storage=MemoryStorage())

dp.update.middleware(DatabaseSessionMiddleware(async_session))
dp.update.middleware(PydanticStateMiddleware(UserStateModel, state_key="user_state"))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AWS
sqs = boto3.client("sqs", region_name="us-east-1")
s3 = boto3.client("s3", region_name="us-east-1")
QUEUE_URL = "your-sqs-queue-url"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π URL
S3_BUCKET = "your-s3-bucket-name"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∏–º—è –≤–∞—à–µ–≥–æ –±–∞–∫–µ—Ç–∞

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF
class PDFStates(StatesGroup):
    waiting_for_text = State()

# –†–æ—É—Ç–µ—Ä –¥–ª—è PDF –∫–æ–º–∞–Ω–¥
pdf_router = Router()

@pdf_router.message(commands=["generate_pdf"])
async def cmd_generate_pdf(message: types.Message, state: FSMContext):
    await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF:")
    await state.set_state(PDFStates.waiting_for_text)

@pdf_router.message(PDFStates.waiting_for_text)
async def process_pdf_text(message: types.Message, state: FSMContext):
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ SQS
        sqs.send_message(
            QueueUrl=QUEUE_URL,
            MessageBody=json.dumps({
                "user_id": message.from_user.id,
                "text": message.text
            })
        )
        await message.reply("PDF –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /showpdf –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∞–π–ª–∞.")
    except Exception as e:
        await message.reply(f"–û—à–∏–±–∫–∞: {str(e)}")
    finally:
        await state.clear()

@pdf_router.message(commands=["showpdf"])
async def cmd_show_pdf(message: types.Message):
    try:
        user_id = message.from_user.id
        s3_key = f"pdfs/{user_id}.pdf"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Å—ã–ª–∫—É
        url = s3.generate_presigned_url(
            'get_object',
            Params={'Bucket': S3_BUCKET, 'Key': s3_key},
            ExpiresIn=3600
        )
        await message.reply(f"–í–∞—à PDF: {url}")
    except Exception as e:
        await message.reply("PDF –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.")

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ä–æ—É—Ç–µ—Ä—ã
dp.include_router(user_creation_router)
dp.include_router(pdf_router)

async def main():
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
---------------------------------------------------------------------
import boto3
import json
from aiogram import Bot, Dispatcher, types
from aiogram.dispatcher import FSMContext
from aiogram.contrib.fsm_storage.memory import MemoryStorage
from aiogram.dispatcher.filters.state import State, StatesGroup

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
bot = Bot(token="YOUR_TELEGRAM_BOT_TOKEN")
storage = MemoryStorage()
dp = Dispatcher(bot, storage=storage)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SQS
sqs = boto3.client("sqs", region_name="us-east-1")
QUEUE_URL = "https://sqs.us-east-1.amazonaws.com/123456789012/pdf-generation-queue"

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è FSM
class PDFStates(StatesGroup):
    waiting_for_text = State()

@dp.message_handler(commands=["generate_pdf"])
async def cmd_generate_pdf(message: types.Message):
    await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ PDF:")
    await PDFStates.waiting_for_text.set()

@dp.message_handler(state=PDFStates.waiting_for_text)
async def process_text(message: types.Message, state: FSMContext):
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤ SQS
        sqs.send_message(
            QueueUrl=QUEUE_URL,
            MessageBody=json.dumps({
                "user_id": message.from_user.id,
                "text": message.text
            })
        )
        await message.reply("–í–∞—à PDF –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /showpdf –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∞–π–ª–∞.")
    except Exception as e:
        await message.reply(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
    finally:
        await state.finish()

if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)
---------------------------------------------------------------
@dp.message_handler(commands=["showpdf"])
async def cmd_show_pdf(message: types.Message):
    s3 = boto3.client("s3")
    user_id = message.from_user.id
    s3_key = f"pdfs/{user_id}.pdf"
    
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ PDF
        url = s3.generate_presigned_url(
            "get_object",
            Params={"Bucket": "my-telegram-pdf-bucket", "Key": s3_key},
            ExpiresIn=3600
        )
        await message.reply(f"–í–∞—à PDF: {url}")
    except Exception as e:
        await message.reply("PDF –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.")

-----------------------------------------------------

import boto3
from aiogram import Bot, Dispatcher, types

bot = Bot(token="YOUR_TELEGRAM_BOT_TOKEN")
sqs = boto3.client("sqs", region_name="us-east-1")
QUEUE_URL = "https://sqs.us-east-1.amazonaws.com/123456789012/pdf-generation-queue"

@dp.message_handler(commands=["generate_pdf"])
async def cmd_generate_pdf(message: types.Message):
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–¥–∞—á—É –≤ SQS
    sqs.send_message(
        QueueUrl=QUEUE_URL,
        MessageBody=json.dumps({
            "user_id": message.from_user.id,
            "text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è PDF"  # –ú–æ–∂–Ω–æ –±—Ä–∞—Ç—å –∏–∑ –ë–î
        })
    )
    await message.reply("PDF –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ /showpdf")

-----------------------------------------------------------------------

resource "aws_lambda_event_source_mapping" "sqs_trigger" {
  event_source_arn = aws_sqs_queue.pdf_generation_queue.arn
  function_name    = aws_lambda_function.pdf_generator.arn
  batch_size       = 1
}

import boto3
from fpdf import FPDF  # pip install fpdf2
import json
import os

s3 = boto3.client("s3")
S3_BUCKET = os.getenv("S3_BUCKET")

def generate_pdf(content, filename):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, txt=content)
    pdf.output(filename)
    return filename

def lambda_handler(event, context):
    for record in event["Records"]:
        body = json.loads(record["body"])
        user_id = body["user_id"]
        text = body["text"]
        
        pdf_filename = f"/tmp/{user_id}_document.pdf"
        generate_pdf(text, pdf_filename)
        
        s3_key = f"pdfs/{user_id}.pdf"
        s3.upload_file(pdf_filename, S3_BUCKET, s3_key)
        
    return {"statusCode": 200}

resource "aws_s3_bucket" "pdf_bucket" {
  bucket = "my-telegram-pdf-bucket"  # –£–∫–∞–∂–∏—Ç–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è
  acl    = "private"
}

resource "aws_s3_bucket_versioning" "pdf_bucket_versioning" {
  bucket = aws_s3_bucket.pdf_bucket.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_iam_role" "lambda_exec_role" {
  name = "lambda-pdf-generator-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Action = "sts:AssumeRole",
      Effect = "Allow",
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_policy" "lambda_s3_sqs_policy" {
  name = "lambda-s3-sqs-policy"

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes"
        ],
        Resource = aws_sqs_queue.pdf_generation_queue.arn
      },
      {
        Effect = "Allow",
        Action = [
          "s3:PutObject",
          "s3:GetObject"
        ],
        Resource = "${aws_s3_bucket.pdf_bucket.arn}/*"
      },
      {
        Effect = "Allow",
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ],
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_policy_attach" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = aws_iam_policy.lambda_s3_sqs_policy.arn
}

resource "aws_sqs_queue" "example_queue" {
  name                      = "example-queue.fifo"  # –î–ª—è FIFO –æ—á–µ—Ä–µ–¥–∏ –¥–æ–±–∞–≤—å—Ç–µ .fifo
  delay_seconds             = 90                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π (0-900)
  max_message_size          = 262144                # –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è (262144 –±–∞–π—Ç = 256 KB)
  message_retention_seconds = 345600                # –•—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π (4 –¥–Ω—è)
  receive_wait_time_seconds = 10                    # –î–æ–ª–≥–∏–π –æ–ø—Ä–æ—Å (0-20)
  visibility_timeout_seconds = 30       

container_definitions = jsonencode([
    {
      name      = "run-migrations"
      image     = "890742572904.dkr.ecr.eu-central-1.amazonaws.com/devops-bot:latest"
      essential = false
      memory    = 256
      command   = ["domigrations"]
      environment = [{
        name  = "POSTGRES_URL"
        value = "${var.db_endpoint}"
      }]

      logConfiguration = {
        logDrive = "awslogs"
        options = {
          awslogs-group         = aws_cloudwatch_log_group.ecs_logs.name
          awslogs-region        = "eu-central-1"
          awslogs-stream-prefix = "ecs"
        }
      }

    },

Error: creating ECS Task Definition (example-task): operation error ECS: RegisterTaskDefinition, 1 validation error(s) found.
‚îÇ - missing required field, RegisterTaskDefinitionInput.ContainerDefinitions[0].LogConfiguration.LogDriver.
‚îÇ 
‚îÇ 
‚îÇ   with module.ecs_cluster.aws_ecs_task_definition.ecs_task,
‚îÇ   on ../modules/ecs_service_atl/main.tf line 36, in resource "aws_ecs_task_definition" "ecs_task":
‚îÇ   36: resource "aws_ecs_task_definition" "ecs_task" {


Error: creating ECS Task Definition (example-task): operation error ECS: RegisterTaskDefinition, https response error StatusCode: 400, RequestID: 573ee6be-6b1b-4bea-a4a2-09993d0eac27, ClientException: Invalid setting for container 'run-migrations'. At least one of 'memory' or 'memoryReservation' must be specified.
‚îÇ 
‚îÇ   with module.ecs_cluster.aws_ecs_task_definition.ecs_task,
‚îÇ   on ../modules/ecs_service_atl/main.tf line 31, in resource "aws_ecs_task_definition" "ecs_task":
‚îÇ   31: resource "aws_ecs_task_definition" "ecs_task" {
‚îÇ 


Error: creating RDS DB Instance (terraform-20250416155251204200000002): operation error RDS: CreateDBInstance, https response error StatusCode: 400, RequestID: c56f1d9d-d2f3-4437-a775-933cb046c68a, api error InvalidParameterValue: Invalid DB engine
‚îÇ 
‚îÇ   with module.newversatl.aws_db_instance.test_db,
‚îÇ   on ../modules/newversatl/main.tf line 121, in resource "aws_db_instance" "test_db":
‚îÇ  121: resource "aws_db_instance" "test_db" {


Error: creating EC2 Launch Template (test-launch-template): operation error EC2: CreateLaunchTemplate, https response error StatusCode: 400, RequestID: 931fd454-9e50-41f6-9263-eb14a9dee523, api error InvalidUserData.Malformed: Invalid BASE64 encoding of user data.
‚îÇ 
‚îÇ   with module.ecs_cluster.aws_launch_template.ecs_lt,
‚îÇ   on ../modules/ecs_service_atl/main.tf line 5, in resource "aws_launch_template" "ecs_lt":
‚îÇ    5: resource "aws_launch_template" "ecs_lt" {
‚îÇ 
‚ïµ
‚ï∑
‚îÇ Error: creating ELBv2 application Load Balancer (atllb): operation error Elastic Load Balancing v2: CreateLoadBalancer, https response error StatusCode: 400, RequestID: 77deac47-f776-473b-8c06-28b05a16514f, api error ValidationError: At least two subnets in two different Availability Zones must be specified
‚îÇ 
‚îÇ   with module.newversatl.aws_lb.atllb["atllb"],
‚îÇ   on ../modules/newversatl/main.tf line 78, in resource "aws_lb" "atllb":
‚îÇ   78: resource "aws_lb" "atllb" {
‚îÇ 
‚ïµ
‚ï∑
‚îÇ Error: creating RDS DB Instance (terraform-20250416153917577000000001): operation error RDS: CreateDBInstance, https response error StatusCode: 400, RequestID: 47d045f8-2b82-4bf5-af06-5d97b75f64b1, InvalidSubnet: No default subnet detected in VPC. Please contact AWS Support to recreate default Subnets.
‚îÇ 
‚îÇ   with module.newversatl.aws_db_instance.test_db,
‚îÇ   on ../modules/newversatl/main.tf line 116, in resource "aws_db_instance" "test_db":
‚îÇ  116: resource "aws_db_instance" "test_db" {


Planning failed. Terraform encountered an error while generating this plan.

‚ï∑
‚îÇ Error: Cycle: module.newversatl.aws_security_group.this["secgrouplb"], module.newversatl.aws_security_group.this["secgroupinstance"], module.newversatl.aws_security_group.this["secgroupdb"]


updating Security Group (sg-0b3669ea628af5c31) ingress rules: authorizing Security Group (ingress) rules: operation error EC2: AuthorizeSecurityGroupIngress, https response error StatusCode: 400, RequestID: 3b487961-a3b6-4833-86fd-d357833aa497, api error InvalidGroupId.Malformed: Invalid id: "secgrouplb" (expecting "sg-...")
‚îÇ 
‚îÇ   with module.newversatl.aws_security_group.this["secgroupinstance"],
‚îÇ   on ../modules/newversatl/main.tf line 41, in resource "aws_security_group" "this":
‚îÇ   41: resource "aws_security_group" "this" {


# –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–∑ Python 3.10
FROM python:3.10-slim

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è Postgres –∏ Redis
RUN apt-get update && apt-get install -y \
    gcc \
    python3-dev \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
WORKDIR /app

# –ö–æ–ø–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Ö
COPY pyproject.toml poetry.lock* /app/
RUN pip install --upgrade pip && \
    pip install poetry && \
    poetry config virtualenvs.create false && \
    poetry install --no-dev --no-root

# –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å—å –ø—Ä–æ–µ–∫—Ç
COPY . .

# –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ `main:app` –Ω–∞ –≤–∞—à —Å–∫—Ä–∏–ø—Ç)
CMD ["poetry", "run", "python", "-m", "your_bot_module.main"]

Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

Traceback (most recent call last):
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 146, in __init__
    self._dbapi_connection = engine.raw_connection()
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3298, in raw_connection
    return self.pool.connect()
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 1263, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 712, in checkout
    rec = pool._do_get()
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 308, in _do_get
    return self._create_connection()
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)


# curl -v http://app-backend:8000
* Host app-backend:8000 was resolved.
* IPv6: (none)
* IPv4: 10.107.183.231
*   Trying 10.107.183.231:8000...
* Connected to app-backend (10.107.183.231) port 8000
> GET / HTTP/1.1
> Host: app-backend:8000
> User-Agent: curl/8.5.0
> Accept: */*
> 
< HTTP/1.1 404 Not Found
< date: Mon, 14 Apr 2025 14:20:05 GMT
< server: uvicorn
< content-length: 22
< content-type: application/js


File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 308, in _do_get
    return self._create_connection()
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
  File "/root/.cache/pypoetry/virtualenvs/atlantis-fastapi-cCrPfAjT-py3.10/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 674, in __init__
    self.__connect()

FROM node:20-alpine AS builder
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm install
COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist/ /usr/share/nginx/html
EXPOSE 80

Back-off restarting failed container frontend in pod app-frontend-5795c45db8-5z9pf_atlantis(08728b66-b7b6-4c90-a214-f8aa3bef7786)

WebSocket connection to 'ws://atlantis.com/?token=Nn8hhDXRC70m' failed: 
createConnection @ client:756Understand this errorAI
client:769 WebSocket connection to 'ws://localhost:5173/?token=Nn8hhDXRC70m' failed: 
createConnection @ client:769Understand this errorAI
client:784 [vite] failed to connect to websocket.
your current setup:
  (browser) atlantis.com/ <--[HTTP]--> localhost:5173/ (server)
  (browser) atlantis.com:/ <--[WebSocket (failing)]--> localhost:5173/ (server)
Check out your Vite / network configuration and 

minikube tunnel
helm upgrade -f values.yaml ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx


0/1 nodes are available: persistentvolumeclaim "app-postgres-pvc" not found. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.


FROM python:3.10
WORKDIR /app
COPY pyproject.toml poetry.lock .
# RUN apt-get update && apt-get upgrade -y
# RUN  curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/
#get-poetry.py | python -
#RUN bash -c "source $HOME/.poetry/env"
RUN pip install poetry
RUN poetry install --no-root
COPY . .
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh
ENTRYPOINT ["/app/entrypoint.sh"]



Back-off restarting failed container backend in pod app-backend-85767b5f7b-rl8cr_atlantis(ebd805d1-9622-4858-a013-b36edb3cfd66)
6
83s
Container image "atl-back:latest" already present on machine
4
87s
Created container: backend
4
87s
Error: failed to start container "backend": Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: exec: "entrypoint.sh": executable file not found in $PATH: unknown
4

OCI runtime exec failed: exec failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown


ERROR [internal] load metadata for docker.io/library/build:latest                                                          0.7s
 => CANCELED [internal] load metadata for docker.io/library/nginx:alpine                                                       0.7s
------
 > [internal] load metadata for docker.io/library/build:latest:
------

 1 warning found (use docker --debug to expand):
 - FromAsCasing: 'as' and 'FROM' keywords' casing do not match (line 1)
Dockerfile:9
--------------------
   7 |     
   8 |     FROM nginx:alpine
   9 | >>> COPY --from=build /app/build /usr/share/nginx/html
  10 |     EXPOSE 80
  11 |     CMD ["nginx", "-g", "daemon off;" ]
--------------------
ERROR: failed to solve: build: failed to resolve source metadata for docker.io/library/build:latest: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed (did you mean builder?)



failed to solve: build: failed to resolve source metadata for docker.io/library/build:latest: pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed (did you mean builder?)


failed to solve: process "/bin/sh -c npm install" did not complete successfully: exit code: 127

nodes are available: persistentvolumeclaim "app-postgres-pvc" not found. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.

Unexpected error obtaining ingress-nginx pod: unable to get POD information (missing POD_NAME or POD_NAMESPACE environment variable


No permissions to list and get Ingress Classes: ingressclasses.networking.k8s.io is forbidden: User "system:serviceaccount:ingress-nginx:nginx-ingress-serviceaccount" cannot list resource "ingressclasses" in API group "networking.k8s.io" at the cluster scope, IngressClass feature will be disabled

apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  type: NodePort  # –ò–ª–∏ LoadBalancer –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
    - name: https
      port: 443
      targetPort: https
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx


rules:
  # –ü—Ä–∞–≤–∏–ª–∞ –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è Ingress-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
  - apiGroups: [""]
    resources: ["configmaps", "endpoints", "nodes", "pods", "secrets"]
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

Error: INSTALLATION FAILED: 2 errors occurred:
	* Deployment in version "v1" cannot be handled as a Deployment: json: cannot unmarshal object into Go struct field Container.spec.template.spec.containers.ports of type []v1.ContainerPort
	* Ingress.networking.k8s.io "app-ingress-rule" is invalid: spec.rules[0].host: Invalid value: "localhost:5173": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')


kubectl exec -it postgres-pod -- bash -c "echo \"listen_addresses = '*'\" >> /var/lib/postgresql/data/postgresql.conf"

bash -c "echo 'host all all 0.0.0.0/0 trust' >> /var/lib/postgresql/data/pg_hba.conf"

telnet: can't connect to remote host (10.109.8.131): Connection refused


until nc -z -v -w30 {{ .Release.Name }}-postgres 5432; do

* Job in version "v1" cannot be handled as a Job: json: cannot unmarshal object into Go struct field PodSpec.spec.template.spec.initContainers of type []v1.Container


Error: INSTALLATION FAILED: unable to build kubernetes objects from release manifest: resource mapping not found for name: "app-migrate" namespace: "" from "": no matches for kind "job" in version "batch/v1beta1"
ensure CRDs are installed first


* Deployment.apps "app-postgres" is invalid: spec.template.spec.containers[0].volumeMounts[0].name: Not found: "postgres-storage"


* Deployment in version "v1" cannot be handled as a Deployment: json: cannot unmarshal object into Go struct field PodSpec.spec.template.spec.containers of type []v1.Container
	* job in version "v1" cannot be handled as a Job: no kind "job" is registered for version "batch/v1" in scheme "pkg/api/legacyscheme/scheme.go:30"

Failed to pull image "frontend": Error response from daemon: pull access denied for frontend, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
db:
    image: postgres:13
    container_name: postgres 
    environment:
      POSTGRES_USER: myuser            
      POSTGRES_PASSWORD: mypassword    
      POSTGRES_DB: mydatabase         
    volumes:
      - ./postgres_data:/var/lib/postgresql/data  
    ports:
      - "5433:5432"  
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "myuser"]
      interval: 10s
      timeout: 5s
      retries: 5

resource "aws_launch_configuration" "ecs_lc" {
  name          = "ecs-launch-config"
  image_id      = "ami-0c55b159cbfafe1f0" # –ó–∞–º–µ–Ω–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π ECS-AMI
  instance_type = "t2.micro"
  security_groups = [aws_security_group.ecs_sg.id]
  user_data = <<EOF
#!/bin/bash
echo "ECS_CLUSTER=${aws_ecs_cluster.ecs_cluster.name}" >> /etc/ecs/ecs.config
EOF
  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "ecs_asg" {
  desired_capacity     = 1
  max_size            = 1
  min_size            = 1
  vpc_zone_identifier = [aws_subnet.subnet.id]
  launch_configuration = aws_launch_configuration.ecs_lc.id
}

resource "aws_ecs_task_definition" "ecs_task" {
  family                   = "example-task"
  network_mode             = "bridge"
  requires_compatibilities = ["EC2"]
  container_definitions = jsonencode([
    {
      name      = "example-container"
      image     = "nginx:latest"
      memory    = 256
      cpu       = 128
      essential = true
      portMappings = [
        {
          containerPort = 80
          hostPort      = 80
        }
      ]
    }
  ])
}

resource "aws_ecs_service" "ecs_service" {
  name            = "example-service"
  cluster         = aws_ecs_cluster.ecs_cluster.id
  task_definition = aws_ecs_task_definition.ecs_task.arn
  desired_count   = 1
  launch_type     = "EC2"
}


ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 1facf49a-7004-4268-a80a-1adc292b6635::94oiepgi8dvdnr9inkp1elvez: "/entrypoint": not found

GPT PMBR size mismatch (16777215 != 25165823) will be corrected by write.
The backup GPT table is not on the end of the device. This problem will be corrected by write.
This disk is currently in use - repartitioning is probably a bad idea.
It's recommended to umount all file systems, and swapoff all swap
partitions on this disk.


sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "db" to address: Temporary failure in name resolution

[build 6/6] RUN npm run build:
0.317 
0.317 > atlantis-typescript@0.0.0 build
0.317 > tsc -b && vite build
0.317 
6.493 src/components/form/Form.tsx(57,44): error TS2339: Property 'data' does not exist on type '{}'.
6.494 src/components/sideBar/SideBar.tsx(1,9): error TS6133: 'Divider' is declared but its value is never read.
6.495 src/components/sideBar/SideBar.tsx(7,1): error TS6133: 'React' is declared but its value is never read.
6.495 src/features/clients/components/ClientDeleteModal.tsx(16,37): error TS7031: Binding element 'onConfirm' implicitly has an 'any' type.
6.495 src/features/clients/components/ClientDeleteModal.tsx(16,48): error TS7031: Binding element 'onCancel' implicitly has an 'any' type.
6.495 src/features/clients/components/ClientsForm.tsx(5,1): error TS6133: 'dayjs' is declared but its value is never read.
6.495 src/features/clients/components/ClientsForm.tsx(74,33): error TS7006: Parameter 'values' implicitly has an 'any' type.
6.497 src/features/clients/components/ClientsForm.tsx(74,41): error TS7031: Binding element 'resetForm' implicitly has an 'any' type.
6.497 src/features/clients/components/ClientsForm.tsx(92,36): error TS2339: Property 'data' does not exist on type '{}'.
6.498 src/features/clients/tables/clientsColumns.ts(3,47): error TS2304: Cannot find name 'rows'.
6.498 src/layouts/Clients.tsx(1,22): error TS6133: 'Divider' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(1,43): error TS6133: 'List' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(1,49): error TS6133: 'ListItem' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(1,59): error TS6133: 'ListItemIcon' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(1,80): error TS6133: 'Typography' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(13,1): error TS6133: 'Checkbox' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(14,1): error TS6133: 'WarningSpan' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(15,1): error TS6133: 'CircleIcon' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(17,1): error TS6133: 'IClientGet' is declared but its value is never read.
6.498 src/layouts/Clients.tsx(43,35): error TS7006: Parameter 'params' implicitly has an 'any' type.
6.498 src/layouts/Clients.tsx(55,13): error TS2304: Cannot find name 'setIsAbleToDelete'.
6.498 src/layouts/Clients.tsx(64,33): error TS7006: Parameter 'e' implicitly has an 'any' type.
6.498 src/layouts/Clients.tsx(76,33): error TS7006: Parameter 'params' implicitly has an 'any' type.
6.498 src/layouts/Clients.tsx(89,9): error TS2304: Cannot find name 'setIsAbleToDelete'.
6.498 src/layouts/Clients.tsx(123,29): error TS2322: Type 'GridRowId' is not assignable to type 'number'.
6.498   Type 'string' is not assignable to type 'number'.
6.500 src/layouts/HomeLayout.tsx(5,9): error TS6133: 'Navigate' is declared but its value is never read.
6.500 src/layouts/HomeLayout.tsx(20,11): error TS6198: All destructured elements are unused.
6.500 src/pages/home.tsx(20,110): error TS2339: Property 'data' does not exist on type 'FetchBaseQueryError | SerializedError'.
6.500   Property 'data' does not exist on type 'SerializedError'.
6.500 src/store/apis/clientsApi.ts(2,35): error TS2307: Cannot find module '../../models/client.ts' or its corresponding type declarations.
------
Dockerfile:6
--------------------
   4 |     RUN npm install
   5 |     COPY . .
   6 | >>> RUN npm run build
   7 |     
   8 |     FROM nginx:alpine
--------------------
ERROR: failed to solve: process "/bin/sh -c npm run build" did not complete successfully: exit code: 2
